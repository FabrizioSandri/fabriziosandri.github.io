---
---

@inproceedings{sandri2025ssp,
    title={2{SSP}: A Two-Stage Framework for Structured Pruning of {LLM}s},
    author={Fabrizio Sandri and Elia Cunegatti and Giovanni Iacca},
    booktitle={Accepted at Sparsity in LLMs (SLLM) Workshop at ICLR 2025},
    abstract={We propose a novel Two-Stage framework for Structured Pruning (2SSP) for pruning Large Language Models (LLMs), which combines two different strategies of pruning, namely Width and Depth Pruning. The first stage (Width Pruning) removes entire neurons, hence their corresponding rows and columns, aiming to preserve the connectivity among the pruned structures in the intermediate state of the Feed-Forward Networks in each Transformer block. The second stage (Depth Pruning), instead, removes entire Attention submodules. We also propose a novel mechanism to balance the sparsity rate of the two stages w.r.t. to the desired global sparsity. We test 2SSP on four LLM families and three sparsity rates (25\%, 37.5\%, and 50\%), measuring the resulting perplexity over three language modeling datasets as well as the performance over six downstream tasks. Our method consistently outperforms five state-of-the-art competitors over three language modeling and six downstream tasks, with an up to two-order-of-magnitude gain in terms of pruning time. 
The code is available at available at <a href="https://github.com/FabrizioSandri/2SSP">https://github.com/FabrizioSandri/2SSP</a>.},
    code={https://github.com/FabrizioSandri/2SSP},
    month = {January},
    year={2025},
    address = {Singapore}, 
    url={https://openreview.net/forum?id=j0NRBl9vVd},
    arxiv={2501.17771},
    pdf={https://arxiv.org/pdf/2501.17771},
    abbr = {SLLM @ ICLR2025},
    selected={true},
}


@article{sandri20252SSP,
    title={2SSP: A Two-Stage Framework for Structured Pruning of LLMs},
    author={Fabrizio Sandri and Elia Cunegatti and Giovanni Iacca},
    abstract={We propose a novel Two-Stage framework for Structured Pruning (2SSP) for pruning Large Language Models (LLMs), which combines two different strategies of pruning, namely Width and Depth Pruning. The first stage (Width Pruning) removes entire neurons, hence their corresponding rows and columns, aiming to preserve the connectivity among the pruned structures in the intermediate state of the Feed-Forward Networks in each Transformer block. The second stage (Depth Pruning), instead, removes entire Attention submodules. We also propose a novel mechanism to balance the sparsity rate of the two stages w.r.t. to the desired global sparsity. We test 2SSP on four LLM families and three sparsity rates (25\%, 37.5\%, and 50\%), measuring the resulting perplexity over three language modeling datasets as well as the performance over six downstream tasks. Our method consistently outperforms five state-of-the-art competitors over three language modeling and six downstream tasks, with an up to two-order-of-magnitude gain in terms of pruning time. 
The code is available at available at <a href="https://github.com/FabrizioSandri/2SSP">https://github.com/FabrizioSandri/2SSP</a>.},
    journal={arXiv preprint arXiv:2501.17771},
    url={https://arxiv.org/abs/2501.17771},
    code={https://github.com/FabrizioSandri/2SSP},
    pdf={https://openreview.net/pdf?id=j0NRBl9vVd},
    year={2025},
    abbr = {Preprint},
    selected={true},
}

